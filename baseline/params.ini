[runtime]
resume = T

; This tells us what sampling method to use to explore the space.
; To start with we can just use the "test" sampler which doesn't
; really explore, but just evaluates the model at a single set of
; parameters and saves the results.

; Once the tests are complete you can change this to "nautilus".
; The configuration options for each sampler are below in their
; own sections.
#sampler = test
sampler = nautilus

verbosity = standard
root = ../cosmosis-standard-library

[pipeline]
; This is a list of calculations that the pipeline does. You can have a look for them
; on the cosmosis documentation pages. Each one refers to a section below in the file,
; which defines what "module" is run. You can add the "heat" module to this list once
; you've written it.
modules =  consistency load_nz     cosmopower astropy  extrapolate   pk_to_cl  2pt_like  

; The second configuration file specified here includes the names and ranges
; of cosmological parameters that will be varied by nautilus
values = ./values.ini

; This makes more error messages be printed out when something goes wrong
debug = T

[nautilus]
; The nautilus docs recommended this as a minimum, but we could try
; increasing it to get more samples if we need to later.
live_points = 1000

[output]
; This is where the output file gets saved.
filename = n_z_real_index_48.txt

[test]
fatal_errors = T

; This will save the results to a directory with this name.
; Everything calculated at each step in the model is saved here.
save_dir = baseline_model

;
; Below this point each section below describes one of the modules.
; The meanings of all the parameters are described on the standard
; library overview documentation page. (The cosmopower module is not
; in the main library yet so is not documented; we are using a side
; branch of the code that includes it).
;

[consistency]
; This first module calculates various derived cosmological parameters
; from the ones that are specified in the values file, basically filling
; in the gaps. e.g. it knows that the Omega parameters have to add up to
; one so computes the missing ones.
file = utility/consistency/consistency_interface.py


[schmear]
file = number_density/photoz_bias/photoz_bias.py
mode = additive
sample = nz_source


[stretch]
file = number_density/photoz_width/photoz_width.py
mode = stretch
sample = nz_source



[cosmopower]
; This is a neural network emulator that estimates the *matter power spectrum*, which
; describes the distribution of matter in 3D across the Universe with redshift.
file = structure/cosmopower/matter_power_camb_sigma8/cosmopower_interface_sigma8.py
path_2_trained_emulator = structure/cosmopower/matter_power_camb_sigma8/emu_files/
use_specific_k_modes = F 
kmax = 10.0 
kmin = 1e-5 
nk = 200 


[astropy]
; This module does exactly what you did in Computer Modelling - it computes
; the expansion history of the Universe, giving us distance as a function
; of redshift.
file = background/astropy_background/astropy_background.py 
model = lambdacdm 
nz = 200 
zmax = 3.0   


[extrapolate]
; The matter power that cosmopower emulates doesn't go down to small enough scales
; for what we need to do. This code extrapolates it to smaller ones.
; (These calculations are done in Fourier space, so this is done my choosing a maximum
; wavenumber k to go to, equivalent to a smaller scale. The numbers are in MegaParsecs).
file = boltzmann/extrapolate/extrapolate_power.py
kmax = 500.



; This file, which I wrote for this project, reads n(z) data from
; a specific file format that Yun-Hao created with his simulations.
; You'll need to download it as noted in the readme.
[load_nz]
file = ${PWD}/../load_redshift_sample/load_redshift_sample.py
source_or_lens = source
; -1 means to use the average redshift sample.
; otherwise we can use an index to load a specific sample
index = 48
filename = ../data/ENSEMBLE_Y1.hdf5



[pk_to_cl]
; This does the integral that I showed you that goes from the 3D matter
; power calculation to the 2D observable quantity, by integrating over
; distance:
file = structure/projection/project_2d.py
ell_min_logspaced = 0.1
ell_max_logspaced = 5.0e5
n_ell_logspaced = 100 
shear-shear = source-source
verbose = F
get_kernel_peaks = F



[2pt_like]
; This compares the resulting model to some mock observed data.
; In this test case we simulated the data ourselves, so
file = likelihood/2pt/2pt_like.py
data_file = ../data/baseline.fits



[heat]
; I haven't written this module yet, just put in a skeleton
; that doesn't work. You'll need to update the file referenced here!
; The PWD stands for "present working directory".
file = ${PWD}/../heat/likelihood_temperature.py
; You will need to fill in this parameter when you're ready:
temperature = 1.0



[save_2pt]
; When I initially created the baseline.fits file I did 
; it by replacing save_2pt in the pipeline at the top with
; save_2pt, which is this one. We might need to run this again
; if we change the baseline n(z).
file = likelihood/2pt/save_2pt.py
; The name of the file to generate:
filename = baseline.fits
overwrite = T
; Choices of what to generate (lensing only):
real_space = F
make_covariance = T
shear_nz_name = nz_source
spectrum_sections = shear_cl
two_thirds_midpoint = F
; These are standard LSST numbers:
ell_min = 20
ell_max = 2000
n_ell = 20
fsky = 0.43477 
number_density_shear_arcmin2 = 2.0  2.0  2.0  2.0  2.0 
sigma_e_total = 0.26  0.26  0.26  0.26  0.26
; This isn't actually used because we are not doing galaxy clustering,
; but it needs to be set or the code complains:
number_density_lss_arcmin2 = 0.0
